# -*- coding: utf-8 -*-
"""DBSCAN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18cT4kdaHYXJBOO9tZUpFKiOxRGM6Hcjv

# A diversified portfolio using Density-based spatial clustering of applications with noise(DBSCAN)
"""

pip install yfinance --upgrade

import pandas as pd
import yfinance as yf
from datetime import datetime
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import normalize
from sklearn.decomposition import PCA
from scipy.stats import skew,kurtosis,norm,skewtest,kurtosistest
from statsmodels.graphics.tsaplots import plot_pacf,plot_acf
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout
from sklearn.metrics import mean_absolute_error
plt.style.use('fivethirtyeight')

stock_list = ['ADANIPORTS.NS','ARVIND.NS', 'ASIANPAINT.NS', 'AXISBANK.NS', 'BAJAJ-AUTO.NS', 'BAJFINANCE.NS', 'BAJAJFINSV.NS', 'BHARTIARTL.NS', 'BPCL.NS', 'BRITANNIA.NS','CIPLA.NS', 'COALINDIA.NS', 'DIVISLAB.NS', 'DRREDDY.NS', 'EICHERMOT.NS', 'GILLETTE.NS', 'GOKEX.NS', 'GRASIM.NS', 'HCLTECH.NS', 'HDFC.NS', 'HDFCBANK.NS', 'HEROMOTOCO.NS', 'HINDALCO.NS', 'HINDUNILVR.NS', 'IBREALEST.NS', 'ICICIBANK.NS', 'INDUSINDBK.NS', 'INFY.NS', 'IOC.NS',  'ITC.NS', 'JSWSTEEL.NS', 'KOTAKBANK.NS', 'LT.NS', 'M&M.NS', 'MARUTI.NS', 'NESTLEIND.NS', 'NTPC.NS', 'ONGC.NS', 'POWERGRID.NS', 'RELIANCE.NS', 'SBIN.NS', 'SBILIFE.NS', 'SHREECEM.NS', 'SUNPHARMA.NS', 'TATAMOTORS.NS', 'TATASTEEL.NS',  'TCS.NS', 'TATACONSUM.NS', 'TECHM.NS', 'TITAN.NS', 'ULTRACEMCO.NS', 'UPL.NS','WIPRO.NS']

stockStartDate = '2010-11-04'
today = datetime.today().strftime('%Y-%m-%d')

data = yf.download(stock_list, stockStartDate, today)['Adj Close']
print(data)

daily_returns = data.pct_change()
annual_mean_returns = daily_returns.mean() * 252
annual_return_variance = daily_returns.var() * 252

stocks = pd.DataFrame(data.columns, columns=['Stock_Symbols'])
stocks['Variances'] = annual_return_variance.values
stocks['Returns'] = annual_mean_returns.values
stocks

X = stocks.drop('Stock_Symbols', axis = 1)
  
# Handling the missing values
X.fillna(method ='ffill', inplace = True)

# Scaling the data to bring all the attributes to a comparable level
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
  
# Normalizing the data so that 
# the data approximately follows a Gaussian distribution
X_normalized = normalize(X_scaled)
  
# Converting the numpy array into a pandas DataFrame
X_normalized = pd.DataFrame(X_normalized)

dbscan = DBSCAN(eps = 0.0255, min_samples =4).fit(X) # fitting the model
labels = dbscan.labels_ # getting the labels
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)
print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)

from sklearn.neighbors import NearestNeighbors # importing the library
neighb = NearestNeighbors(n_neighbors=2) # creating an object of the NearestNeighbors class
nbrs=neighb.fit(X) # fitting the data to the object
distances,indices=nbrs.kneighbors(X) # finding the nearest neighbours
# Sort and plot the distances results
distances = np.sort(distances, axis = 0) # sorting the distances
distances = distances[:, 1] # taking the second column of the sorted distances
plt.rcParams['figure.figsize'] = (5,3) # setting the figure size
plt.plot(distances) # plotting the distancesplt.title('Elbow Curve')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia or Sum Squared Error (SSE)')
plt.show()

dbscan = DBSCAN(eps = 0.0255, min_samples =4).fit(X) # fitting the model
labels = dbscan.labels_
labels

stocks['Cluster_Labels'] = labels
stocks

# Plot the clusters
plt.scatter(X.iloc[:, 0], X.iloc[:,1], c = labels, cmap= "plasma") # plotting the clusters
plt.xlabel("Returns") # X-axis label
plt.ylabel("Variances") # Y-axis label
plt.show() # showing the plot

"""# Portfolio Construction"""

pip install yahoo_fin --upgrade

pip install requests_html

import yahoo_fin.stock_info as si

nif_stats = {}
for ticker in stock_list:
    temp = si.get_stats_valuation(ticker)
    temp = temp.iloc[:,:2]
    temp.columns = ["Attribute", "Recent"]
    nif_stats[ticker] = temp
nif_stats

combined_stats = pd.concat(nif_stats)
combined_stats = combined_stats.reset_index()
combined_stats

del combined_stats["level_1"]
combined_stats.columns = ["Ticker", "Attribute", "Recent"]
combined_stats

pe = combined_stats[combined_stats["Attribute"]=="Forward P/E"].reset_index()
pe

at = pe["Recent"]
stocks = stocks.join(at)
stocks

stocks.columns = ["Stock_Symbols", "Variances",	"Returns",	"Cluster_Labels",	"Forward P/E"]
stocks

finance = stocks.groupby('Cluster_Labels')[['Stock_Symbols','Returns']].max()
finance

port = pd.merge(finance, stocks, how="left", on=["Cluster_Labels", "Stock_Symbols", "Returns"])
port

"""#Portfolio Optimization"""

invest = port["Stock_Symbols"].to_list()
invest

weights = np.array([0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1])

my_stocks = yf.download(invest, stockStartDate, today)['Adj Close']
my_stocks

for c in my_stocks.columns.values:
  plt.plot(my_stocks[c], label = c)

#plt.title(title)
plt.xlabel('Date')
plt.ylabel('Adj. Price')
plt.legend(my_stocks.columns.values, loc = 'upper left')
plt.show()

returns = my_stocks.pct_change()
returns

cov_matrix_annual = returns.cov() * 252
cov_matrix_annual

pip install PyPortfolioOpt

from pypfopt.efficient_frontier import  EfficientFrontier
from pypfopt import risk_models
from pypfopt import expected_returns

mu = expected_returns.mean_historical_return(my_stocks)
S = risk_models.sample_cov(my_stocks)
ef = EfficientFrontier(mu, S)
weight = ef.max_sharpe()
clean_weights = ef.clean_weights()
print(clean_weights)
ef.portfolio_performance(verbose = True)

from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices
latest_prices = get_latest_prices(my_stocks)
weight = clean_weights
da = DiscreteAllocation(weight, latest_prices, total_portfolio_value = 25000)
allocation, leftover = da.lp_portfolio()
print('Discrete allocation: ', allocation)
print('Funds remaining: {:.2f}'.format(leftover))

"""# Statistical Analysis"""

from scipy.stats import skew,kurtosis,norm,skewtest,kurtosistest
from statsmodels.graphics.tsaplots import plot_pacf,plot_acf
import pandas.util.testing as tm

x = pd.DataFrame(my_stocks)
returns = X.pct_change(1).dropna()

plt.hist(returns,bins="rice",label="Daily close price")
plt.legend()
plt.show()

stock = max(port["Returns"])

plt.boxplot(stock, labels=["Daily close price"])
plt.show()

np.mean(returns)

np.std(returns)

np.quantile(returns,0.5)

skew(returns)

skewtest(returns)

kurtosis(returns)

plt.plot(returns)
plt.xlabel("Time")
plt.ylabel("Daily returns")
plt.show()

plt.plot(returns.rolling(20).std())
plt.xlabel("Time")
plt.ylabel("20-days rolling standard deviation")
plt.show()

"""# ARIMA model """

import yfinance as yf
import pandas as pd
spy = yf.Ticker("SPY")
# get stock info
spy.info
# get historical market data as df
hist = spy.history(period="max")
# Save df as CSV
hist.to_csv('SPY.csv')
# Read back in as dataframe
spy = pd.read_csv('SPY.csv')
# Convert Date column to datetime
spy['Date'] = pd.to_datetime(spy['Date'])

x_valid = spy.loc[(spy['Date'] < test_split_date) & (spy['Date'] > train_split_date)]['Close']

max (port['Returns'])

df_new = stocks.query("Returns==0.4942714261137792")
comapny = df_new["Stock_Symbols"].iloc[0]

df_new=port.query("Returns== 0.4942714077238606") 
comapny= df_new['Stock_Symbols'].iloc[0]

ticker = yf.Ticker(comapny)# get stock info
ticker.info# get historical market data as df
hist = ticker.history(period="max")# Save df as CSV
hist.to_csv('ticker.csv')# Read back in as dataframe
ticker = pd.read_csv('ticker.csv')# Convert Date column to datetime
ticker['Date'] = pd.to_datetime(ticker['Date'])

# Set target series
series = ticker['Close']# Create train data set
train_split_date = '2010-11-04'
train_split_index = np.where(ticker.Date == train_split_date)[0][0]
x_train = ticker.loc[ticker['Date'] <= train_split_date]['Close']# Create test data set
test_split_date = '2018-01-02'
test_split_index = np.where(ticker.Date == test_split_date)[0][0]
x_test = ticker.loc[ticker['Date'] >= test_split_date]['Close']# Create valid data set
valid_split_index = (train_split_index.max(),test_split_index.min())
x_valid = ticker.loc[(ticker['Date'] < test_split_date) & (ticker['Date'] > train_split_date)]['Close']#printed index values are: 
#0-5521(train), 5522-6527(valid), 6528-6947(test)

def test_stationarity(timeseries, window = 12, cutoff = 0.01):

    #Determing rolling statistics
    rolmean = timeseries.rolling(window).mean()
    rolstd = timeseries.rolling(window).std()

    #Plot rolling statistics:
    fig = plt.figure(figsize=(12, 8))
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean &amp; Standard Deviation')
    plt.show()
    
    #Perform Dickey-Fuller test:
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    pvalue = dftest[1]
    if pvalue < cutoff:
        print('p-value = %.4f. The series is likely stationary.' % pvalue)
    else:
        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)
    
    print(dfoutput)

from statsmodels.tsa.stattools import adfuller
def test_stationarity(timeseries, window = 12, cutoff = 0.01):

    #Determing rolling statistics
    rolmean = timeseries.rolling(window).mean()
    rolstd = timeseries.rolling(window).std()

    #Plot rolling statistics:
    fig = plt.figure(figsize=(12, 8))
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean &amp; Standard Deviation')
    plt.show()
    
    #Perform Dickey-Fuller test:
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    pvalue = dftest[1]
    if pvalue < cutoff:
        print('p-value = %.4f. The series is likely stationary.' % pvalue)
    else:
        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)
    
    print(dfoutput)

test_stationarity(series)

# Get the difference of each Adj Close point
spy_close_diff_1 = series.diff()# Drop the first row as it will have a null value in this column
spy_close_diff_1.dropna(inplace=True)

test_stationarity(spy_close_diff_1)

from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
plot_acf(spy_close_diff_1)
plt.xlabel('Lags (Days)')
plt.show()# Break these into two separate cells
plot_pacf(spy_close_diff_1)
plt.xlabel('Lags (Days)')
plt.show()

# Use this block to
from statsmodels.tsa.arima_model import ARIMA# fit model
spy_arima = ARIMA(x_train, order=(1,1,1))
spy_arima_fit = spy_arima.fit(disp=0)
print(spy_arima_fit.summary())

# Plot our predictions against the actual values for a visual comparison.
model_predictions = []
plt.plot(x_test.index[-20:], model_predictions[-20:], color='blue',label='Predicted Price')
plt.plot(x_test.index[-20:], x_test[-20:], color='red', label='Actual Price')
plt.ylabel('Dollars $')
plt.xlabel('Timestep in Days')
plt.title('ARIMA(4,2,0) Forecast vs Actual')
# plt.xticks(np.arange(881,1259,50), df.Date[881:1259:50])
plt.legend()
plt.figure(figsize=(10,6))
plt.show()

"""# Predict Stock Prices With LSTMs"""

from pandas_datareader import data
import matplotlib.pyplot as plt
import pandas as pd
import datetime as dt
import urllib.request, json
import os
import numpy as np
import tensorflow as tf # This code has been tested with TensorFlow 1.6
from sklearn.preprocessing import MinMaxScaler

from pandas_datareader.data import DataReader

import pandas as pd
import matplotlib.pyplot as plt
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

#downloading historical data
df = yf.download(invest, stockStartDate, today)
df

y = df.copy()
print(y.shape)
print(y.tail())

y.corr()

split_ratio = 0.2
y = y.values # Convert to NumPy array
split = int(len(y) * (1-split_ratio))
train_set = y[: split]
test_set = y[split:]
print(train_set.shape, test_set.shape)

def supvervisedSeries(df, n, h):
  p, q = list (), list ()
  for i in range (len(df)-n-h+1):
    p.append(df[i:(i+n)])
    q.append(df[i+h+n-1])
  return np.array(p), np.array(q)
h = 1
n = 4
trainX, trainY = supvervisedSeries(train_set, n, h)
testX, testY = supvervisedSeries(test_set, n, h)
print("trainX: ", trainX.shape)
print("trainY: ", trainY.shape)
print("testX: ", testX.shape)
print("testY: ", testY.shape)

testY = np.reshape(testY[:, 0], (testY [:, 0].shape[0], 1))
trainY = np.reshape(trainY[:, 0], (trainY[:, 0].shape[0], 1))
print("trainY: ", trainY.shape)
print("testY: ", testY.shape)

scalers = {}
for i in range (trainX.shape[2]):
  scalers[i] = MinMaxScaler()
  trainX[:, :, i] = scalers[i].fit_transform(trainX[:, :, i])
for i in range(testX.shape[2]):
  testX[:, :, i] = scalers[i].transform(testX[:, :, i])
# The target values are 2D arrays, which is easy to scale
scalerY = MinMaxScaler()
trainY = scalerY.fit_transform(trainY)
testY = scalerY.transform(testY)

# Flatten input (to support multivariate input)
n_input = trainX.shape[1] * trainX.shape[2]
trainX = trainX.reshape((trainX.shape[0], n_input))
n_input = testX.shape[1] * testX.shape[2]
testX = testX.reshape((testX.shape[0], n_input))
# Create multilayered FFNN model
model = Sequential()
model.add(Dense(100, activation='relu', input_dim=trainX.shape[1]))
model.add(Dropout(0.2))
model.add(Dense(100, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(100, activation='relu'))
model.add(Dense(trainY.shape[1]))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()
# Fit model
history = model.fit(trainX, trainY, epochs =60, verbose =1)
# Predict the test set
predictions = model.predict(testX)

plt.figure(figsize=(15,6))
plt.plot(predictions, label="Test set predictions" )
plt.plot(testY, label="Real data")
plt.legend()
plt.ylabel('Price Index')
plt.xlabel('time step' )
plt.show()