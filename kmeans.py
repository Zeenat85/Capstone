# -*- coding: utf-8 -*-
"""KMeans

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sm9BterMbnDIDgxlaj5psjhVcuKQOIGJ

# K-Means Clustering to build a Diversified Portfolio
In the management of a financial portfolio one important consideration is the correlations between the portfolio's various stocks. For this analysis we will make use of an unsupervised machine learning technique called kmeans. This technique uses vectors on data points. These data points are not labeled or classified. Our goal is to discover hidden patterns and group the data points in a sensible way based on similarity of features. Each group of data points is a cluster and each cluster will have a center.
"""

pip install yfinance

#importing libraries
import pandas as pd
import yfinance as yf
from datetime import datetime
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
from scipy.stats import skew,kurtosis,norm,skewtest,kurtosistest
from statsmodels.graphics.tsaplots import plot_pacf,plot_acf
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout
from sklearn.metrics import mean_absolute_error

#creating a list for stock symbols/stickers
stock_list = ['ADANIPORTS.NS','ARVIND.NS', 'ASIANPAINT.NS', 'AXISBANK.NS', 'BAJAJ-AUTO.NS', 'BAJFINANCE.NS', 'BAJAJFINSV.NS', 'BHARTIARTL.NS', 'BPCL.NS', 'BRITANNIA.NS','CIPLA.NS', 'COALINDIA.NS', 'DIVISLAB.NS', 'DRREDDY.NS', 'EICHERMOT.NS', 'GILLETTE.NS', 'GOKEX.NS', 'GRASIM.NS', 'HCLTECH.NS', 'HDFC.NS', 'HDFCBANK.NS', 'HEROMOTOCO.NS', 'HINDALCO.NS', 'HINDUNILVR.NS', 'IBREALEST.NS', 'ICICIBANK.NS', 'INDUSINDBK.NS', 'INFY.NS', 'IOC.NS',  'ITC.NS', 'JSWSTEEL.NS', 'KOTAKBANK.NS', 'LT.NS', 'M&M.NS', 'MARUTI.NS', 'NESTLEIND.NS', 'NTPC.NS', 'ONGC.NS', 'POWERGRID.NS', 'RELIANCE.NS', 'SBIN.NS', 'SBILIFE.NS', 'SHREECEM.NS', 'SUNPHARMA.NS', 'TATAMOTORS.NS', 'TATASTEEL.NS',  'TCS.NS', 'TATACONSUM.NS', 'TECHM.NS', 'TITAN.NS', 'ULTRACEMCO.NS', 'UPL.NS','WIPRO.NS']

#start and end date for obtaining historical data
stockStartDate = '2010-11-04'
today = datetime.today().strftime('%Y-%m-%d')

#downloading historical data
data = yf.download(stock_list, stockStartDate, today)['Adj Close']
print(data)

#calculating daily_returns, mean and variance of annual returns
daily_returns = data.pct_change()
annual_mean_returns = daily_returns.mean() * 252
annual_return_variance = daily_returns.var() * 252

#storing them in a dataframe
stocks = pd.DataFrame(data.columns, columns=['Stock_Symbols'])
stocks['Variances'] = annual_return_variance.values
stocks['Returns'] = annual_mean_returns.values
stocks

#plotting elbow curve to determine the number of clusters
x = stocks[['Returns', 'Variances']].values
inertia_list = []
for k in range(2, 53):
  kmeans = KMeans(n_clusters=k)
  kmeans.fit(x)
  inertia_list.append(kmeans.inertia_)

plt.plot(range(2,53), inertia_list)
plt.title('Elbow Curve')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia or Sum Squared Error (SSE)')
plt.show()

pip install kneed

#number of clusters
from kneed import KneeLocator
clusters_iterations = range(2, 53)
kl = KneeLocator(clusters_iterations, inertia_list, curve="convex", direction="decreasing")
elbow=kl.elbow
print('Elbow = {}'.format(elbow))

#cluster labels
kmeans = KMeans(n_clusters=elbow).fit(x)
labels = kmeans.labels_
labels

#adding cluster labels to dataframe
stocks['Cluster_Labels'] = labels
stocks

#importing additional libraries
import seaborn as sns

#plotting the kmeans cluster graph
facet = sns.lmplot(data=stocks, x='Returns', y='Variances', hue='Cluster_Labels', fit_reg=False, legend=True)

#determing the cluster centres
clusters_centers_df=pd.DataFrame(kmeans.cluster_centers_,columns=['Returns','Variances'])
clusters_centers_df

y_kmeans = kmeans.predict(x)
clustering_result=pd.DataFrame(zip(y_kmeans,stocks.index),columns=['Cluster','Company'])
clustering_result.set_index('Cluster').head()

for cluster_num in list(clustering_result.set_index('Cluster').index.unique()):
    print(clustering_result.set_index('Cluster').loc[cluster_num].head())

clusters_centers_df['Count']=clustering_result['Cluster'].value_counts().to_frame().rename(columns={'Cluster':'Count'})['Count']
clusters_centers_df.head()

#visualizing count of elements by cluster 
plt.figure(figsize=(11,7))
plt.bar(clusters_centers_df.index.values,clusters_centers_df['Count'])
plt.title("Count of Elements by Cluster")
plt.show()

"""# Portfolio Construction
From each cluster, choose the stocks with the highest Returns.
"""

pip install yahoo_fin --upgrade

import yahoo_fin.stock_info as si

temp = {}
for ticker in stock_list:
    temp[ticker] = si.get_quote_table(ticker)["Market Cap"]
df = pd.DataFrame(temp, index=stock_list)
df_tr = df.transpose()
markcap = df_tr.iloc[: , :1]
markcap.index.name = "Stock_Symbols"
markcap.columns = ['Market Cap']
markcap

markcap[["Market Cap"]] = markcap[["Market Cap"]].apply(lambda x: x.str.replace('.','', regex=True))
markcap[["Market Cap"]] = markcap[["Market Cap"]].apply(lambda x: x.str.replace('T','0000000000', regex=True))
markcap[["Market Cap"]] = markcap[["Market Cap"]].apply(lambda x: x.str.replace('B','0000000', regex=True))
markcap

markcap[["Market Cap"]] = markcap[["Market Cap"]].apply(pd.to_numeric)

markcap['Class'] = np.where(markcap['Market Cap'] >= 200000000000, 'Large-cap', 'Mid-cap')
markcap

finance = stocks.groupby("Cluster_Labels")[["Stock_Symbols", "Returns"]].max()
finance

port = pd.merge(finance, markcap, how="left", on=["Stock_Symbols"])
port

"""# Portfolio Optimization
The objective is to build your portfolio to yield the maximum possible return while maintaining the amount of risk you’re willing to carry.
"""

invest = finance["Stock_Symbols"].to_list()
invest

weights = np.array([0.09, 0.09, 0.09, 0.09, 0.09,0.09, 0.09, 0.09, 0.09, 0.09, 0.01])

my_stocks = yf.download(invest, stockStartDate, today)['Adj Close']
my_stocks

for c in my_stocks.columns.values:
  plt.plot(my_stocks[c], label = c)

#plt.title(title)
plt.xlabel('Date')
plt.ylabel('Adj. Price')
plt.legend(my_stocks.columns.values, loc = 'upper left')
plt.show()

returns = my_stocks.pct_change()
returns

cov_matrix_annual = returns.cov() * 252
cov_matrix_annual

port_variance = np.dot(weights.T, np.dot(cov_matrix_annual, weights))
port_variance

port_volatility = np.sqrt(port_variance)
port_volatility

portfolioSimpleAnnualReturn = np.sum(returns.mean() * weights) * 252
portfolioSimpleAnnualReturn

percent_var = str(round(port_variance, 2) * 100) + '%'
percent_vols = str(round(port_volatility, 2) * 100) + '%'
percent_ret = str(round(portfolioSimpleAnnualReturn, 2) * 100) + '%'
print('Expected annual return: '+  percent_ret)
print('Annual volatility / risk: '+ percent_vols)
print('Annual variance: '+ percent_var)

pip install PyPortfolioOpt

from pypfopt.efficient_frontier import  EfficientFrontier
from pypfopt import risk_models
from pypfopt import expected_returns

mu = expected_returns.mean_historical_return(my_stocks)
S = risk_models.sample_cov(my_stocks)
ef = EfficientFrontier(mu, S)
weight = ef.max_sharpe()
clean_weights = ef.clean_weights()
print(clean_weights)
ef.portfolio_performance(verbose = True)

from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices
latest_prices = get_latest_prices(my_stocks)
weight = clean_weights
da = DiscreteAllocation(weight, latest_prices, total_portfolio_value = 25000)
allocation, leftover = da.lp_portfolio()
print('Discrete allocation: ', allocation)
print('Funds remaining: {:.2f}'.format(leftover))

"""# Statistical Analysis"""

# calculate returns of all the stocks in the portfoio
x = pd.DataFrame(my_stocks)
returns = x.pct_change(1).dropna()

# raw histogram of returns
plt.hist(returns,bins="rice",label="Daily close price")
plt.show()

# maximum return
stock = max(stocks["Returns"])
stock

# boxplot
plt.boxplot(stock, labels=["Daily close price"])
plt.show()

np.mean(returns)

np.std(returns)

np.quantile(returns,0.5)

skew(returns)

skewtest(returns)

kurtosis(returns)

# raw time series
plt.plot(returns)
plt.xlabel("Time")
plt.ylabel("Daily returns")
plt.show()

# 20-days rolling standard deviation
plt.plot(returns.rolling(20).std())
plt.xlabel("Time")
plt.ylabel("20-days rolling standard deviation")
plt.show()

"""# Build an ARIMA Model to Predict a Stock’s Price
ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. 
"""

max(port['Returns'])

df_new = stocks.query("Returns==0.48035645262737636")
comapny = df_new["Stock_Symbols"].iloc[0]

ticker = yf.Ticker(comapny)# get stock info
ticker.info# get historical market data as df
hist = ticker.history(period="max")# Save df as CSV
hist.to_csv('ticker.csv')# Read back in as dataframe
ticker = pd.read_csv('ticker.csv')# Convert Date column to datetime
ticker['Date'] = pd.to_datetime(ticker['Date'])

# Set target series
series = ticker['Close']# Create train data set
train_split_date = '2010-11-04'
train_split_index = np.where(ticker.Date == train_split_date)[0][0]
x_train = ticker.loc[ticker['Date'] <= train_split_date]['Close']# Create test data set
test_split_date = '2018-01-02'
test_split_index = np.where(ticker.Date == test_split_date)[0][0]
x_test = ticker.loc[ticker['Date'] >= test_split_date]['Close']# Create valid data set
valid_split_index = (train_split_index.max(),test_split_index.min())
x_valid = ticker.loc[(ticker['Date'] < test_split_date) & (ticker['Date'] > train_split_date)]['Close']#printed index values are: 
#0-5521(train), 5522-6527(valid), 6528-6947(test)

from statsmodels.tsa.stattools import adfuller
def test_stationarity(timeseries, window = 12, cutoff = 0.01):

    #Determing rolling statistics
    rolmean = timeseries.rolling(window).mean()
    rolstd = timeseries.rolling(window).std()

    #Plot rolling statistics:
    fig = plt.figure(figsize=(12, 8))
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show()
    
    #Perform Dickey-Fuller test:
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    pvalue = dftest[1]
    if pvalue < cutoff:
        print('p-value = %.4f. The series is likely stationary.' % pvalue)
    else:
        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)
    
    print(dfoutput)

test_stationarity(series)

# Get the difference of each Adj Close point
spy_close_diff_1 = series.diff()# Drop the first row as it will have a null value in this column
spy_close_diff_1.dropna(inplace=True)

test_stationarity(spy_close_diff_1)

from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
plot_acf(spy_close_diff_1)
plt.xlabel('Lags (Days)')
plt.show()# Break these into two separate cells
plot_pacf(spy_close_diff_1)
plt.xlabel('Lags (Days)')
plt.show()

# Use this block to
from statsmodels.tsa.arima_model import ARIMA# fit model
spy_arima = ARIMA(x_train, order=(1,1,1))
spy_arima_fit = spy_arima.fit(disp=0)
print(spy_arima_fit.summary())

# Create list of x train valuess
history = [x for x in x_train]
# establish list for predictions
model_predictions = []
# Count number of test data points
N_test_observations = len(x_test)
# loop through every data point
for time_point in list(x_test.index):
    model = ARIMA(history, order=(1,1,1))
    model_fit = model.fit(disp=0)
    output = model_fit.forecast()
    yhat = output[0]
    model_predictions.append(yhat)
    true_test_value = x_test[time_point]
    history.append(true_test_value)
MAE_error = keras.metrics.mean_absolute_error(x_test, model_predictions).numpy()
print('Testing Mean Squared Error is {}'.format(MAE_error))

# Plot our predictions against the actual values for a visual comparison.
plt.plot(x_test.index[-20:], model_predictions[-20:], color='blue',label='Predicted Price')
plt.plot(x_test.index[-20:], x_test[-20:], color='red', label='Actual Price')
plt.ylabel('Rupees')
plt.xlabel('Timestep in Days')
plt.title('ARIMA(4,2,0) Forecast vs Actual')
# plt.xticks(np.arange(881,1259,50), df.Date[881:1259:50])
plt.legend()
plt.figure(figsize=(10,6))
plt.show()

"""# Predict Stock Prices With LSTMs
Long-Short-Term-Memory (LSTM) networks are a type of neural network commonly used to predict time series data. In simple words, they have a memory/cache functionality which helps them learn the long term dependencies and relations in the data. So, looking at the previous N data points, they can predict the next (or next few) points by learning the patterns.
"""

#downloading historical data
df = yf.download(invest, stockStartDate, today)

y = df.copy()
print(y.shape)
print(y.tail())

y.corr()

split_ratio = 0.2
y = y.values # Convert to NumPy array
split = int(len(y) * (1-split_ratio))
train_set = y[: split]
test_set = y[split:]
print(train_set.shape, test_set.shape)

def supvervisedSeries(df, n, h):
  p, q = list (), list ()
  for i in range (len(df)-n-h+1):
    p.append(df[i:(i+n)])
    q.append(df[i+h+n-1])
  return np.array(p), np.array(q)
h = 1
n = 4
trainX, trainY = supvervisedSeries(train_set, n, h)
testX, testY = supvervisedSeries(test_set, n, h)
print("trainX: ", trainX.shape)
print("trainY: ", trainY.shape)
print("testX: ", testX.shape)
print("testY: ", testY.shape)

testY = np.reshape(testY[:, 0], (testY [:, 0].shape[0], 1))
trainY = np.reshape(trainY[:, 0], (trainY[:, 0].shape[0], 1))
print("trainY: ", trainY.shape)
print("testY: ", testY.shape)

scalers = {}
for i in range (trainX.shape[2]):
  scalers[i] = MinMaxScaler()
  trainX[:, :, i] = scalers[i].fit_transform(trainX[:, :, i])
for i in range(testX.shape[2]):
  testX[:, :, i] = scalers[i].transform(testX[:, :, i])
# The target values are 2D arrays, which is easy to scale
scalerY = MinMaxScaler()
trainY = scalerY.fit_transform(trainY)
testY = scalerY.transform(testY)

# Flatten input (to support multivariate input)
n_input = trainX.shape[1] * trainX.shape[2]
trainX = trainX.reshape((trainX.shape[0], n_input))
n_input = testX.shape[1] * testX.shape[2]
testX = testX.reshape((testX.shape[0], n_input))
# Create multilayered FFNN model
model = Sequential()
model.add(Dense(100, activation='relu', input_dim=trainX.shape[1]))
model.add(Dropout(0.2))
model.add(Dense(100, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(100, activation='relu'))
model.add(Dense(trainY.shape[1]))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()
# Fit model
history = model.fit(trainX, trainY, epochs =60, verbose =1)
# Predict the test set
predictions = model.predict(testX)

plt.figure(figsize=(15,6))
plt.plot(predictions, label="Test set predictions" )
plt.plot(testY, label="Real data")
plt.legend()
plt.ylabel('Price Index')
plt.xlabel('time step' )
plt.title ("Adj close Price prediction")
plt.show()